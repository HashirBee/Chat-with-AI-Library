{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HuggingFace embeddings from disk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\test\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#INDEX_FILE_PATH = \"./faiss_index\"\n",
    "EMBEDDINGS_PATH = \"./huggingface_bge_embeddings\"\n",
    "\n",
    "#Loading HuggingFace Embeddings\n",
    "model_name = \"BAAI/bge-large-en\"\n",
    "\n",
    "def get_embeddings(EMBEDDINGS_PATH, model_name):\n",
    "    if not os.path.exists(EMBEDDINGS_PATH):\n",
    "        print(\"Downloading HuggingFace embeddings...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        tokenizer.save_pretrained(EMBEDDINGS_PATH)\n",
    "        model.save_pretrained(EMBEDDINGS_PATH)\n",
    "    else:\n",
    "        print(\"Loading HuggingFace embeddings from disk...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(EMBEDDINGS_PATH)\n",
    "        model = AutoModel.from_pretrained(EMBEDDINGS_PATH)\n",
    "    return HuggingFaceBgeEmbeddings(cache_folder=EMBEDDINGS_PATH, model_name=model_name)\n",
    "\n",
    "\n",
    "embeddings = get_embeddings(EMBEDDINGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone()\n",
    "\n",
    "def create_pc_index(index_name,dimension=1024):   \n",
    "    pc = Pinecone()\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=dimension,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(    \n",
    "                cloud='aws', \n",
    "                region='us-east-1'\n",
    "            ) \n",
    "        ) \n",
    "\n",
    "index_name = \"genai-library\"\n",
    "create_pc_index(index_name, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding docs to PINECONE index\n",
    "PDFs_folder_path = \"D:\\\\GITHUB\\\\ChatBot_with_MultipleSources_OpenSource_LLMs\\\\data\"\n",
    "\n",
    "def vector_store(PDFs_folder_path, index_name,embeddings):\n",
    "    print(\"Adding Docs to PINECONE index...\")\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    pdf_files = [f for f in os.listdir(PDFs_folder_path)]\n",
    "    loaders = [PDFPlumberLoader(os.path.join(PDFs_folder_path, file)) for file in pdf_files]\n",
    "    docs = []\n",
    "    count = 0\n",
    "    for loader in loaders:\n",
    "        count+=1\n",
    "        print(f\"Adding doc# {count}\")\n",
    "        docs = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        docs = text_splitter.split_documents(docs[:20])\n",
    "        vectorstore.add_documents(docs)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Docs to PINECONE index...\n",
      "Adding doc# 1\n",
      "Adding doc# 2\n",
      "Adding doc# 3\n",
      "Adding doc# 4\n"
     ]
    }
   ],
   "source": [
    "vectorstore = vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading HuggingFace Embeddings\n",
    "# def get_embeddings():\n",
    "#     if not os.path.exists(EMBEDDINGS_MODEL_PATH):\n",
    "#         print(\"Downloading HuggingFace embeddings...\")\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         model = AutoModel.from_pretrained(model_name)\n",
    "#         tokenizer.save_pretrained(EMBEDDINGS_MODEL_PATH)\n",
    "#         model.save_pretrained(EMBEDDINGS_MODEL_PATH)\n",
    "#     else:\n",
    "#         print(\"Loading HuggingFace embeddings from disk...\")\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(EMBEDDINGS_MODEL_PATH)\n",
    "#         model = AutoModel.from_pretrained(EMBEDDINGS_MODEL_PATH)\n",
    "#     return HuggingFaceBgeEmbeddings(cache_folder=EMBEDDINGS_MODEL_PATH, model_name=model_name)\n",
    "\n",
    "\n",
    "#INDEX_FILE_PATH = \"./faiss_index\"\n",
    "# def vector_store():\n",
    "#     print(\"Creating PINECONE index...\")\n",
    "#     if index_name not in pc.list_indexes().names():\n",
    "#         pc.create_index(\n",
    "#             name=index_name,\n",
    "#             dimension=1536,\n",
    "#             metric=\"cosine\",\n",
    "#             spec=ServerlessSpec(    \n",
    "#                 cloud='aws', \n",
    "#                 region='us-east-1'\n",
    "#             ) \n",
    "#         ) \n",
    "#     pdf_files = [f for f in os.listdir(PDFs_folder_path)]\n",
    "#     loaders = [PDFPlumberLoader(os.path.join(PDFs_folder_path, file)) for file in pdf_files]\n",
    "#     docs = []\n",
    "#     for loader in loaders:\n",
    "#         docs.extend(loader.load())\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "#     docs = text_splitter.split_documents(docs)\n",
    "#     vectorstore = PineconeVectorStore.from_documents(\n",
    "#         docs,\n",
    "#         index_name=index_name,\n",
    "#         embedding=embeddings\n",
    "#     )\n",
    "#     return vectorstore\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
